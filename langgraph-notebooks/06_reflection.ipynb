{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Sequence\n",
    "\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import END, MessageGraph\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from.env file\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\",\n",
    "                        stop_sequences=\"[end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a chat prompt template\n",
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            '''You're a Twitter expert tasked with crafting exceptional tweets. \n",
    "            Your mission is to create the most engaging and impactful tweet possible based on the user's request.\n",
    "            If the user provides feedback, you'll refine and enhance your previous attempts to maximize engagement and deliver outstanding results.''',\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "generation_chain  = generation_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ''\n",
    "request = HumanMessage(\n",
    "    content='Olymics 2024'\n",
    ")\n",
    "\n",
    "for chunk in generation_chain.stream({'messages': [request]}): \n",
    "    print(chunk.content, end='')\n",
    "    tweet += chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            '''You're a highly influential Twitter personality renowned for crafting captivating content and sharing astute observations.\n",
    "            Review and critique the userâ€™s tweet.\n",
    "            To effectively refine the work, you should provide constructive feedback that targets key areas for improvement, such as increasing its depth, refining its style, and amplifying its overall impact.\n",
    "            You can make the tweet more compelling and engaging for their audience by offering specific suggestions.'''\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='messages'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reflection_chain  = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = ''\n",
    "\n",
    "for chunk in reflection_chain.stream(\n",
    "    {'messages': [request, HumanMessage(content=tweet)]}\n",
    "):\n",
    "    print(chunk.content, end='')\n",
    "    reflection += chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflectin using MessageGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ''\n",
    "request = HumanMessage(\n",
    "    content='Olymics 2024'\n",
    ")\n",
    "generation_chain.invoke({'messages': [request]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[request]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    return generation_chain.invoke([state])\n",
    "\n",
    "def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    role_map = {'ai': HumanMessage, 'human': AIMessage}\n",
    "\n",
    "    updated_messages = [messages[0]] + [role_map[msg.type](content=msg.content) for msg in messages[1:]]\n",
    "    response = reflection_chain.invoke({'messages': updated_messages})\n",
    "    return HumanMessage(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = MessageGraph()\n",
    "\n",
    "graph_builder.add_node('generate', generation_node)\n",
    "graph_builder.add_node('reflect', reflection_node)\n",
    "graph_builder.set_entry_point('generate')\n",
    "# graph_builder.set_finish_point('generate')\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return 'reflect'\n",
    "\n",
    "graph_builder.add_conditional_edges('generate', should_continue)\n",
    "graph_builder.add_edge('reflect', 'generate')\n",
    "\n",
    "message_graph = graph_builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
